# -*- coding: utf-8 -*-
"""TASK1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1g2oNX2GGe2Fe5W03ah4JU7us0tRbJx5k
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.decomposition import PCA
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import joblib


CSV_PATH = "Iris.csv"
TEST_SIZE = 0.2
RANDOM_STATE = 42
DO_PLOTS = True


df = pd.read_csv("/content/Iris.csv")
print("Shape:", df.shape)
print("Columns:", df.columns.tolist())
print(df.head())


for c in ['Id', 'id', 'Unnamed: 0']:
    if c in df.columns:
        df = df.drop(columns=[c])
        print(f"Dropped column: {c}")

# Ensure Species column exists
if 'Species' not in df.columns and 'species' in df.columns:
    df = df.rename(columns={'species':'Species'})

# 2) EDA
print("\nUnique species:", df['Species'].unique())
print(df.describe())

if DO_PLOTS:
    feature_cols = [c for c in df.columns if c != 'Species']
    for col in feature_cols:
        plt.figure(figsize=(6,3))
        plt.hist(df[col])
        plt.title(f'Histogram: {col}')
        plt.xlabel(col)
        plt.ylabel('count')
        plt.show()
        plt.figure(figsize=(6,3))
        plt.boxplot(df[col], vert=False)
        plt.title(f'Boxplot: {col}')
        plt.show()

    # Scatter one plot and PCA plot
    if len(feature_cols) >= 2:
        xcol, ycol = feature_cols[0], feature_cols[1]
        le_tmp = LabelEncoder()
        y_tmp = le_tmp.fit_transform(df['Species'])
        plt.figure(figsize=(6,4))
        plt.scatter(df[xcol], df[ycol], c=y_tmp)
        plt.xlabel(xcol); plt.ylabel(ycol)
        for i, sp in enumerate(le_tmp.classes_):
            plt.scatter([], [], label=sp)
        plt.legend()
        plt.title(f"{xcol} vs {ycol}")
        plt.show()

    # PCA 2 components
    X_all = df[[c for c in df.columns if c != 'Species']].values
    pca = PCA(n_components=2, random_state=RANDOM_STATE)
    Xp = pca.fit_transform(X_all)
    plt.figure(figsize=(6,4))
    plt.scatter(Xp[:,0], Xp[:,1], c=le_tmp.transform(df['Species']))
    plt.title('PCA (2 components)')
    plt.xlabel('PC1'); plt.ylabel('PC2')
    for i, sp in enumerate(le_tmp.classes_):
        plt.scatter([], [], label=sp)
    plt.legend()
    plt.show()

# Prepare data for modeling
features = [c for c in df.columns if c != 'Species']
X = df[features].values
le = LabelEncoder()
y = le.fit_transform(df['Species'].values)
print("Labels mapping:", dict(zip(le.classes_, le.transform(le.classes_))))

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y
)
scaler = StandardScaler()
X_train_s = scaler.fit_transform(X_train)
X_test_s = scaler.transform(X_test)

# Baseline models
models = {
    "LogisticRegression": LogisticRegression(max_iter=300, random_state=RANDOM_STATE),
    "KNN": KNeighborsClassifier(),
    "DecisionTree": DecisionTreeClassifier(random_state=RANDOM_STATE),
    "RandomForest": RandomForestClassifier(random_state=RANDOM_STATE)
}

results = {}
for name, model in models.items():
    model.fit(X_train_s, y_train)
    y_pred = model.predict(X_test_s)
    acc = accuracy_score(y_test, y_pred)
    print(f"\n{name} test accuracy: {acc:.4f}")
    print(classification_report(y_test, y_pred, target_names=le.classes_, zero_division=0))
    print("Confusion matrix:\n", confusion_matrix(y_test, y_pred))
    results[name] = acc

# Cross-validation for RandomForest
rf = RandomForestClassifier(random_state=RANDOM_STATE)
cv_scores = cross_val_score(rf, scaler.transform(X), y, cv=5, n_jobs=-1)
print("\nRandomForest CV scores:", np.round(cv_scores, 4))
print("Mean CV accuracy:", cv_scores.mean())

#  GridSearch (small) for RandomForest (fast)
param_grid = {
    "n_estimators": [50, 100],
    "max_depth": [None, 3]
}
grid = GridSearchCV(RandomForestClassifier(random_state=RANDOM_STATE), param_grid, cv=5, n_jobs=-1)
grid.fit(X_train_s, y_train)
print("\nBest Grid params:", grid.best_params_)
best_rf = grid.best_estimator_
y_best = best_rf.predict(X_test_s)
print("Best RF test accuracy:", accuracy_score(y_test, y_best))
print(classification_report(y_test, y_best, target_names=le.classes_, zero_division=0))
print("Confusion matrix:\n", confusion_matrix(y_test, y_best))

#  Feature importances
if hasattr(best_rf, "feature_importances_"):
    importances = best_rf.feature_importances_
    print("Feature importances:")
    for f,v in zip(features, importances):
        print(f" {f}: {v:.4f}")

#  Save: scaler + model + label encoder + feature names
pipeline_obj = {"scaler": scaler, "model": best_rf, "label_encoder": le, "features": features}
joblib.dump(pipeline_obj, "iris_best_pipeline.joblib")
print("\nSaved pipeline to iris_best_pipeline.joblib")